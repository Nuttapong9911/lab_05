{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nuttapong9911/lab_05/blob/main/Hand_signs_detecting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **<font color='rgb(248, 113, 113)'>Project:</font> <font color='pink'>Hand signs detecting. ✋ </font>**\n",
        "\n",
        "เป็นโปรเจคเกี่ยวกับการทํานายภาษามือ โดยจะให้โมเดลเรียนรู้ภาษามือหรือลักษณะของมือรูปแบบ ต่างๆ ผ่านการป้อนรูปภาพที่เป็นภาษามือรูปแบบต่างๆ เพื่อให้โมเดลสามารถ ตรวจจับและจําแนก ลักษณะท่าทางของมือได้ และ โปรเจคของกลุ่มเราสามารถนาไปใช้ต่อยอดได้ในหลายๆเรื่อง ตัวอย่าง เช่น การสร้างระบบสังการอัตโนมัติทคอยรับคําสั่งจากท่าทางของเราเพื่อทําการปิดไฟห้องเมื่อจะนอน (ไม่ต้องลุกไปปัดเอง) และน่าไปสร้างเป็นแอพพลิเคชั่น ที่สามารถช่วยให้คนปกติทั่วไปสามารถคุยกับผู้ พิการทางการได้ยินได้ง่ายขึ้น"
      ],
      "metadata": {
        "id": "onlGqfL-p6el"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import libraries**"
      ],
      "metadata": {
        "id": "jFzAT8yXWzEY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skEPNEQXptP_"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# from torch.utils.data import random_split, DataLoader\n",
        "# import torch.optim as optim\n",
        "# import torchvision\n",
        "%reset -f\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from pathlib import Path\n",
        "\n",
        "import os\n",
        "from skimage import io, img_as_float\n",
        "\n",
        "# from __future__ import print_function\n",
        "import argparse\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Load Data From Kaggle**\n",
        "Kaggle's Link: [hand-signs-images](https://www.kaggle.com/datasets/ash2703/handsignimages)"
      ],
      "metadata": {
        "id": "YZEB0cLUAPBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install kaggle"
      ],
      "metadata": {
        "id": "ColonHRIAZiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "83FUBT-TEjw3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce61499a-d4cf-496e-d33e-f4844319eacc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "t_fIlHnpZfSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/ColabNotebooks/kaggle.json ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "24rT6N_RZ5T7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "SfU6KSA1b4Ti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle datasets download ash2703/handsignimages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9p0IXHHcWAC",
        "outputId": "a51ed0c4-fedd-4984-e1a3-f671f7fa69a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading handsignimages.zip to /content\n",
            "\r  0% 0.00/23.1M [00:00<?, ?B/s]\r 39% 9.00M/23.1M [00:00<00:00, 77.5MB/s]\n",
            "\r100% 23.1M/23.1M [00:00<00:00, 142MB/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip handsignimages.zip"
      ],
      "metadata": {
        "id": "o4rjksFwWYo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Find mean and sd of training data**\n",
        "\n",
        "❌**no need to run again**"
      ],
      "metadata": {
        "id": "jSUFKFlal-i9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean_arr = np.array([])\n",
        "sd_arr = np.array([])\n",
        "\n",
        "for subdir, dirs, files in os.walk('/content/drive/MyDrive/dummy'):\n",
        "    for file in files:\n",
        "        # print (os.path.join(subdir, file))\n",
        "        filepath = subdir + os.sep + file\n",
        "        if filepath.endswith(\".jpg\"):\n",
        "            image = io.imread(filepath)\n",
        "            image = img_as_float(image)\n",
        "            mean_arr = np.append(mean_arr, image)\n",
        "            sd_arr = np.append(sd_arr, image)\n",
        "            # print (filepath)\n"
      ],
      "metadata": {
        "id": "ZFZ3ZLyJgWar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(mean_arr))\n",
        "print(len(sd_arr))\n",
        "print(np.mean(mean_arr))\n",
        "print(np.std(sd_arr))\n",
        "x = np.array([np.mean(mean_arr)])\n",
        "print(len(x))\n",
        "y = np.array([np.std(sd_arr)])\n",
        "print(len(y))"
      ],
      "metadata": {
        "id": "_oK_OAGzgXD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Find Dimension for use in Linear\n",
        "❌No need to run"
      ],
      "metadata": {
        "id": "ag1xI4ztKmd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model1 = nn.Sequential(\n",
        "#     nn.Conv2d(1, 32, kernel_size=3),\n",
        "#     nn.ReLU(),\n",
        "#     nn.MaxPool2d(kernel_size=2),\n",
        "#     nn.Conv2d(32, 64, kernel_size=3),\n",
        "#     nn.ReLU(),\n",
        "#     nn.MaxPool2d(kernel_size=2),\n",
        "#     nn.Dropout(0.25),\n",
        "#     nn.Flatten(),\n",
        "#     # nn.Linear(1600, 128), \n",
        "# )\n",
        "\n",
        "# print('Dimension for fc1: ')\n",
        "# model1(torch.zeros(128,1,28,28)).size()\n",
        "\n",
        "model2 = nn.Sequential(\n",
        "    nn.Conv2d(1, 32, kernel_size=3),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2),\n",
        "    nn.Conv2d(32, 64, kernel_size=3),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2),\n",
        "    nn.Dropout(0.25),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(1600, 128), \n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    # nn.Linear(128, 128), \n",
        ")\n",
        "\n",
        "print('Dimension for fc2: ')\n",
        "model2(torch.zeros(128,1,28,28)).size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9QlCdB1J57E",
        "outputId": "eb14f53f-7c26-4f85-a518-e3b4d2af4a1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimension for fc2: \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([128, 128])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Initial setting**"
      ],
      "metadata": {
        "id": "mcZQrpbSWajv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- <font color='yellow'>initial device setting</font>"
      ],
      "metadata": {
        "id": "NsyWfrsJg834"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(1600, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output"
      ],
      "metadata": {
        "id": "OO_CII0V_YjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- <font color='yellow'>initial parameter setting </font>"
      ],
      "metadata": {
        "id": "HX5u7kB6ig4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = {\n",
        "    '--tain-batch-size' : 128,\n",
        "    '--test-batch-size' : 128,\n",
        "    '--epochs' : 15,\n",
        "    '--lr' : 0.697,\n",
        "    '--gamma' : 0.7,\n",
        "    '--no-cuda' : True,\n",
        "    '--no-mps' : True,\n",
        "    '--seed' : 1\n",
        "}"
      ],
      "metadata": {
        "id": "RJSnlXfEhdzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Prepare The Data**"
      ],
      "metadata": {
        "id": "qSs0ToZhsw4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = torchvision.transforms.Compose([ \n",
        "                  torchvision.transforms.ToTensor(),\n",
        "                  torchvision.transforms.Normalize((0.6246,), (0.1911,)),\n",
        "                  torchvision.transforms.Grayscale(num_output_channels =1)\n",
        "])\n",
        "\n",
        "ds_train = torchvision.datasets.ImageFolder('/content/Train',\n",
        "                                      transform=transform)\n",
        "\n",
        "ds_test  = torchvision.datasets.ImageFolder('/content/Test',   \n",
        "                                      transform=transform)\n",
        "# print(len(ds_train))\n",
        "\n",
        "ds_train, ds_val = random_split(ds_train, [21964,5491])\n",
        "                                    \n",
        "dl_train = DataLoader(ds_train, batch_size= args['--tain-batch-size'], shuffle=True)\n",
        "dl_val   = DataLoader(ds_val, batch_size= args['--tain-batch-size'])\n",
        "dl_test  = DataLoader(ds_test, batch_size= args['--test-batch-size'])"
      ],
      "metadata": {
        "id": "6Cvn6FXds1ur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### fix bug: FileNotFoundError: Found no valid file for the classes .ipynb_checkpoints\n",
        "\n",
        "❌ **ถ้าไม่ บัค ก็ไม่ต้องรัน**"
      ],
      "metadata": {
        "id": "yDGw4nlPs9pT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! rm -R /content/Train/.ipynb_checkpoints\n",
        "! ls /content/Train \n",
        "! rm -R /content/Test/.ipynb_checkpoints\n",
        "! ls /content/Test"
      ],
      "metadata": {
        "id": "XuT4uaNws-YU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing batch size, channels, dimensions"
      ],
      "metadata": {
        "id": "lFasekoFtA2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_features, train_labels = next(iter(dl_train))\n",
        "print(f\"Feature batch shape: {train_features.size()}\")\n",
        "print(f\"Labels batch shape: {train_labels.size()}\")\n",
        "\n",
        "# test if data is loaded correctly\n",
        "img = train_features[1].squeeze()\n",
        "label = train_labels[1]\n",
        "plt.imshow(img, cmap=\"gray\")\n",
        "plt.show()\n",
        "print(f\"Label: {label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "AbjNKtZCtK0r",
        "outputId": "243c3e40-a699-4cd9-fc48-e286b783f91a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature batch shape: torch.Size([128, 1, 28, 28])\n",
            "Labels batch shape: torch.Size([128])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train function**"
      ],
      "metadata": {
        "id": "jFwIGXC2YhEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(args, model, device, train_loader, optimizer, epoch, loss_fn):\n",
        "    trn_loss = 0.0    \n",
        "    correct = 0\n",
        "\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "      # print(f'batch_idx: {batch_idx}, data: {data}, target: {target}')\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = loss_fn(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        trn_loss += loss.item()\n",
        "        pred = output.argmax(dim=1, keepdim=True)          \n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    print(f'Epoch {epoch}: Train loss: {trn_loss/len(dl_train):8.5f}, Train acc: {100*correct/len(dl_train.dataset):6.2f}%')"
      ],
      "metadata": {
        "id": "TuA3x143YgXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Valid function**"
      ],
      "metadata": {
        "id": "doSfIpyPQOrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def valid(args, model, device, dl_val, loss_fn):  \n",
        "  val_loss = 0.0\n",
        "  val_correct = 0\n",
        "\n",
        "  model.eval()\n",
        "  val_correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data, target in dl_val:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        output = model(data)                    \n",
        "        loss = loss_fn(output, target)\n",
        "\n",
        "        val_loss += loss.item()\n",
        "\n",
        "        pred = output.argmax(dim=1, keepdim=True)  \n",
        "        val_correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "        \n",
        "    print(f'\\t Valid loss: {val_loss/len(dl_val):8.5f}, Valid acc: {100*val_correct/len(dl_val.dataset):6.2f}%')"
      ],
      "metadata": {
        "id": "M5KaFfJlQscc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Test function**"
      ],
      "metadata": {
        "id": "3DZ8IDvg1l33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, device, dl_test, loss_fn):\n",
        "  model.eval()\n",
        "  test_correct = 0\n",
        "  for data, target in dl_test:\n",
        "      data, target = data.to(device), target.to(device)\n",
        "      # output = nn.functional.one_hot(target, num_classes=24).float()  \n",
        "\n",
        "      output = model(data)                        \n",
        "      pred = output.argmax(dim=1, keepdim=True)  \n",
        "      test_correct += pred.eq(target.view_as(pred)).sum().item() \n",
        "\n",
        "  print(f'\\t Test acc: {100*test_correct/len(dl_test.dataset):6.2f}%')  "
      ],
      "metadata": {
        "id": "FtlcIbmM1pkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Main function**"
      ],
      "metadata": {
        "id": "WOKgzbyqWQqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = not args['--no-cuda'] and torch.cuda.is_available()\n",
        "use_mps = not args['--no-mps'] and torch.backends.mps.is_available()\n",
        "\n",
        "# print(f'use_cuda: {use_cuda} , use_mps: {use_mps}')\n",
        "\n",
        "if use_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "elif use_mps:\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "# print(device)\n",
        "\n",
        "model = Net().to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adadelta(model.parameters(), lr=args['--lr'])\n",
        "\n",
        "scheduler = StepLR(optimizer, step_size=1, gamma=args['--gamma'])\n",
        "for epoch in range(1, args['--epochs'] + 1):\n",
        "    train(args, model, device, dl_train, optimizer, epoch, loss_fn)\n",
        "    valid(args, model, device, dl_val, loss_fn)\n",
        "    test(model, device, dl_test, loss_fn)\n",
        "    scheduler.step()\n",
        "\n",
        "# if args.save_model:\n",
        "#     torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jpM6GARhWIFg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "068edf3f-d5e9-4afc-ebbe-2335cb6cee53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train loss:  1.45255, Train acc:  56.70%\n",
            "\t Valid loss:  0.29253, Valid acc:  90.69%\n",
            "\t Test acc:  82.35%\n",
            "Epoch 2: Train loss:  0.27352, Train acc:  90.93%\n",
            "\t Valid loss:  0.04261, Valid acc:  99.54%\n",
            "\t Test acc:  92.97%\n",
            "Epoch 3: Train loss:  0.12836, Train acc:  95.98%\n",
            "\t Valid loss:  0.01614, Valid acc:  99.80%\n",
            "\t Test acc:  93.75%\n",
            "Epoch 4: Train loss:  0.08105, Train acc:  97.50%\n",
            "\t Valid loss:  0.00493, Valid acc: 100.00%\n",
            "\t Test acc:  95.01%\n",
            "Epoch 5: Train loss:  0.06203, Train acc:  98.12%\n",
            "\t Valid loss:  0.00346, Valid acc: 100.00%\n",
            "\t Test acc:  94.98%\n",
            "Epoch 6: Train loss:  0.05083, Train acc:  98.60%\n",
            "\t Valid loss:  0.00220, Valid acc: 100.00%\n",
            "\t Test acc:  95.62%\n",
            "Epoch 7: Train loss:  0.04705, Train acc:  98.59%\n",
            "\t Valid loss:  0.00186, Valid acc: 100.00%\n",
            "\t Test acc:  96.22%\n",
            "Epoch 8: Train loss:  0.04230, Train acc:  98.79%\n",
            "\t Valid loss:  0.00157, Valid acc: 100.00%\n",
            "\t Test acc:  96.10%\n",
            "Epoch 9: Train loss:  0.04068, Train acc:  98.84%\n",
            "\t Valid loss:  0.00139, Valid acc: 100.00%\n",
            "\t Test acc:  95.79%\n",
            "Epoch 10: Train loss:  0.03801, Train acc:  98.94%\n",
            "\t Valid loss:  0.00131, Valid acc: 100.00%\n",
            "\t Test acc:  95.97%\n",
            "Epoch 11: Train loss:  0.03815, Train acc:  98.88%\n",
            "\t Valid loss:  0.00127, Valid acc: 100.00%\n",
            "\t Test acc:  96.04%\n",
            "Epoch 12: Train loss:  0.03801, Train acc:  98.95%\n",
            "\t Valid loss:  0.00124, Valid acc: 100.00%\n",
            "\t Test acc:  95.89%\n",
            "Epoch 13: Train loss:  0.03591, Train acc:  98.92%\n",
            "\t Valid loss:  0.00119, Valid acc: 100.00%\n",
            "\t Test acc:  95.84%\n",
            "Epoch 14: Train loss:  0.03758, Train acc:  98.88%\n",
            "\t Valid loss:  0.00116, Valid acc: 100.00%\n",
            "\t Test acc:  95.90%\n",
            "Epoch 15: Train loss:  0.03642, Train acc:  99.02%\n",
            "\t Valid loss:  0.00114, Valid acc: 100.00%\n",
            "\t Test acc:  95.96%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluate the trained model on test data**"
      ],
      "metadata": {
        "id": "T2PX6jBFvuRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test(model, device, dl_test, loss_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYWiAOF5tcfx",
        "outputId": "7ba3723f-e17f-457e-ea61-d11688973570"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t Test acc:  95.96%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Reference**\n",
        "- https://github.com/pytorch/examples/blob/main/mnist/main.py\n",
        "- https://colab.research.google.com/drive/10W_e4U7nmN9ON4MsivRFIeHv67tH6wBL?usp=sharing&fbclid=IwAR26vlAGxuaVOXv6NfG3Aln4j-HPnHkOLzk1878rHESGd4pJtQMdI8A_sOw#scrollTo=jt_qnYd1ZO_i"
      ],
      "metadata": {
        "id": "mjTLf9Dl9D7a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **สมาชิกในกลุ่ม**\n",
        "- นายคมสัน ทรวงแก้ว ุ630610720\n",
        "- นายเทวฤทธิ์ สมฤทธิ์ 630610731\n",
        "- นายนัฐพงษ์ บุญสละ 630610744"
      ],
      "metadata": {
        "id": "i-GoXjIA9Kk6"
      }
    }
  ]
}